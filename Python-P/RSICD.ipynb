{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dLWsqco31zI"
      },
      "outputs": [],
      "source": [
        "# !pip install nltk -q\n",
        "# !pip install rouge -q\n",
        "# !pip install git+https://github.com/ruotianluo/cider.git\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge import Rouge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-brhuy34MY2",
        "outputId": "322a264f-063f-42de-c6e4-87e18d11b230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5Model(\n",
              "  (shared): Embedding(32128, 512)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-5): 5 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-5): 5 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "from transformers import T5Tokenizer, T5Model\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Replace VGG16 with ResNet50\n",
        "base_model = torchvision.models.resnet50(pretrained=True)\n",
        "base_model = nn.Sequential(*list(base_model.children())[:-1])  # Remove the last layer\n",
        "\n",
        "\n",
        "# Replace LSTM with T5 transformer\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "t5_model = T5Model.from_pretrained('t5-small')\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "base_model.to(device)\n",
        "t5_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2iNozVa4VjE",
        "outputId": "91523f20-4f12-41d3-bc42-e856719a8d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V17maPm4sOK"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load the JSON file\n",
        "with open('/content/drive/MyDrive/Remote_Sensing_Image_Analysis/dataset/RSICD/dataset_rsicd.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Prepare data for TXT\n",
        "txt_data = []\n",
        "\n",
        "# Add headers to the text data\n",
        "txt_data.append(\"images,sentences\")\n",
        "\n",
        "# Iterate through each entry\n",
        "for entry in data['images']:\n",
        "    filename = entry['filename']\n",
        "    sentences = [sent['raw'] for sent in entry['sentences'][:5]]\n",
        "    sentences = [sent.rstrip() for sent in sentences]  # Remove trailing whitespace\n",
        "\n",
        "    # Append filename and sentences to txt_data\n",
        "    for sentence in sentences:\n",
        "        txt_data.append(f\"{filename},{sentence}\")\n",
        "\n",
        "# Write data to TXT file\n",
        "txt_file = 'captions.txt'\n",
        "with open(txt_file, 'w') as f:\n",
        "    # Write data\n",
        "    f.write('\\n'.join(txt_data))\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Remote_Sensing_Image_Analysis/sys/evl.csv\")\n",
        "\n",
        "# Define the column names\n",
        "columns = ['epoch', 'dataset', 'Bleu1', 'Bleu2', 'Bleu3', 'Bleu4', 'Meteor', 'Rouge', 'Cider']\n",
        "\n",
        "# Rename columns of the DataFrame\n",
        "df.columns = columns\n",
        "\n",
        "# Function to filter and print results based on epoch and dataset name\n",
        "def eval(epochs, dataset):\n",
        "    filtered_data = df[(df['epoch'] == epochs) & (df['dataset'] == dataset)]\n",
        "    if not filtered_data.empty:\n",
        "        filtered_data_str = filtered_data[['Bleu1', 'Bleu2', 'Bleu3', 'Bleu4', 'Meteor', 'Rouge', 'Cider']].to_string(index=False)\n",
        "        print(filtered_data_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMr1gn-8449P"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = '/content/drive/MyDrive/Remote_Sensing_Image_Analysis/dataset/RSICD'\n",
        "WORKING_DIR = '/content/drive/MyDrive/Remote_Sensing_Image_Analysis'\n",
        "dataset=\"RSICD\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgE9AkNS49ZW"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet50_Weights\n",
        "\n",
        "# Set device (GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Load pre-trained ResNet50 model\n",
        "base_model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "\n",
        "\n",
        "# Restructure the model\n",
        "# Remove the last layer (fully connected) and use the second last layer's output\n",
        "model = nn.Sequential(*list(base_model.children())[:-1])\n",
        "\n",
        "\n",
        "# Move the model to the device (GPU if available)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ||  One Time Only ||\n",
        "\n",
        "\n",
        "# from torchvision import models, transforms\n",
        "# from PIL import Image\n",
        "# from tqdm import tqdm\n",
        "# import os\n",
        "\n",
        "# # extract features from image\n",
        "# features = {}\n",
        "# directory = os.path.join(BASE_DIR,'RSICD_images')\n",
        "\n",
        "# # Define the preprocessing transform\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize(224),\n",
        "#     transforms.CenterCrop(224),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "# for img_name in tqdm(os.listdir(directory)):\n",
        "#     # load the image from file\n",
        "#     img_path = os.path.join(directory, img_name)\n",
        "#     image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "#     # Preprocess the image\n",
        "#     image = transform(image)\n",
        "#     image = image.unsqueeze(0)\n",
        "\n",
        "#     # Move image to the same device as your model\n",
        "#     image = image.to(device)\n",
        "\n",
        "#     # Extract features (assuming your model is named 'model')\n",
        "#     with torch.no_grad():\n",
        "#         feature = model(image)\n",
        "\n",
        "#     # Convert feature tensor to numpy array and remove batch dimension\n",
        "#     features[img_name] = feature.cpu().numpy().squeeze(0)"
      ],
      "metadata": {
        "id": "ssfFeGhcWtof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0HpVVnZ6G0l"
      },
      "outputs": [],
      "source": [
        "# store features in pickle\n",
        "# with open(os.path.join(WORKING_DIR, 'RSICD_RESNET50_features.pkl'), 'wb') as f:\n",
        "#     pickle.dump(features, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPLkqb_l95sd"
      },
      "outputs": [],
      "source": [
        "# load features from pickle\n",
        "with open(os.path.join(WORKING_DIR, 'RSICD_RESNET50_features.pkl'), 'rb') as f:\n",
        "    features = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOPVkPjr-H4q"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join('captions.txt'), 'r') as f:\n",
        "    next(f)\n",
        "    captions_doc = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create mapping of image to captions\n",
        "mapping = {}\n",
        "for line in tqdm(captions_doc.split('\\n')):\n",
        "    if len(line) < 2:\n",
        "        continue\n",
        "    tokens = line.split(',')\n",
        "    image_id, caption = tokens[0], tokens[1:]\n",
        "    image_id = image_id.split('.')[0]\n",
        "    caption = \", \".join(caption).strip()\n",
        "\n",
        "    if image_id not in mapping:\n",
        "        mapping[image_id] = []\n",
        "    mapping[image_id].append(caption)\n",
        "\n",
        "# Optional: Convert features to PyTorch tensors\n",
        "features = {image_id: torch.tensor(feature) for image_id, feature in features.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S41R1K4obSAM",
        "outputId": "5d7926f3-cdaa-4f97-9009-3bdff80ae30b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 54605/54605 [00:00<00:00, 382869.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPYVliQS-xd2",
        "outputId": "15f7fda4-5ae4-408c-e99e-b318df323f95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10921"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V9LYJuw-zIH"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean(mapping):\n",
        "    for key, captions in mapping.items():\n",
        "        for i in range(len(captions)):\n",
        "            caption = captions[i]\n",
        "            # Convert to lowercase\n",
        "            caption = caption.lower()\n",
        "            # Remove digits, special characters, and non-alphabetic words\n",
        "            caption = re.sub(r'[^a-z\\s]', '', caption)\n",
        "            # Replace multiple spaces with a single space\n",
        "            caption = re.sub(r'\\s+', ' ', caption)\n",
        "            # Remove short words (length 1)\n",
        "            caption = ' '.join([word for word in caption.split() if len(word) > 1])\n",
        "            # Add start and end tags\n",
        "            caption = 'startseq ' + caption + ' endseq'\n",
        "            captions[i] = caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pC64yECU-9Lp",
        "outputId": "a06d9a7e-0d72-45f5-e35a-99ebc030175d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a huge field with trees and plants surrounded while a couple of small playgrounds .',\n",
              " 'an oval court and two-way street nearby .',\n",
              " 'many buildings and some green trees are around three playgrounds in different sizes .',\n",
              " 'a large surrounded is surroundededed by green trees and two small playgrounds is surroundededed by buildings .',\n",
              " 'three playgrounds are surrounded by many trees and buildings .']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# before preprocess of text\n",
        "mapping['00005']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvbZMEcw_NC_"
      },
      "outputs": [],
      "source": [
        "clean(mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML2r42em_PAp",
        "outputId": "df6ba4ce-2356-4849-8583-b10a70ddcc22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['startseq huge field with trees and plants surrounded while couple of small playgrounds endseq',\n",
              " 'startseq an oval court and twoway street nearby endseq',\n",
              " 'startseq many buildings and some green trees are around three playgrounds in different sizes endseq',\n",
              " 'startseq large surrounded is surroundededed by green trees and two small playgrounds is surroundededed by buildings endseq',\n",
              " 'startseq three playgrounds are surrounded by many trees and buildings endseq']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# before preprocess of text\n",
        "mapping['00005']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqtOcYsz_nXX"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "vocab_size = tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCONSfvS_zS6"
      },
      "outputs": [],
      "source": [
        "max_length = 128  # or any other suitable value\n",
        "tokenizer.max_length = max_length"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_ids = list(mapping.keys())\n",
        "train_size = int(len(image_ids) * 0.90)\n",
        "train_ids, test_ids = torch.utils.data.random_split(image_ids, [train_size, len(image_ids) - train_size])\n",
        "\n",
        "train = train_ids\n",
        "test = test_ids"
      ],
      "metadata": {
        "id": "LhYRfNf0cZD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
        "    while True:\n",
        "        for i in range(0, len(train), batch_size):\n",
        "            batch_ids = train[i:i+batch_size]\n",
        "            batch_caps = []\n",
        "            for id in batch_ids:\n",
        "                captions = mapping[id]\n",
        "                batch_caps.extend(captions)\n",
        "            batch_seq = tokenizer.batch_encode_plus(batch_caps,\n",
        "                                                     max_length=max_length,\n",
        "                                                     padding='max_length',\n",
        "                                                     truncation=True,\n",
        "                                                     return_tensors='pt')\n",
        "            yield batch_seq['input_ids'].squeeze()"
      ],
      "metadata": {
        "id": "qKUYAxGDcj-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7YFYnPGs4P9",
        "outputId": "445ba428-b477-45b7-e27e-84aa44d68135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "Batch processing time: 12.80 seconds\n",
            "Batch processing time: 29.05 seconds\n",
            "Batch processing time: 43.42 seconds\n",
            "Batch processing time: 56.99 seconds\n",
            "Batch processing time: 70.36 seconds\n",
            "Batch processing time: 83.68 seconds\n",
            "Batch processing time: 96.85 seconds\n",
            "Batch processing time: 110.30 seconds\n",
            "Batch processing time: 123.93 seconds\n",
            "Batch processing time: 137.00 seconds\n",
            "Batch processing time: 150.24 seconds\n",
            "Batch processing time: 163.91 seconds\n",
            "Batch processing time: 177.94 seconds\n",
            "Batch processing time: 191.73 seconds\n",
            "Batch processing time: 205.20 seconds\n",
            "Batch processing time: 217.79 seconds\n",
            "Batch processing time: 230.67 seconds\n",
            "Batch processing time: 244.11 seconds\n",
            "Batch processing time: 257.54 seconds\n",
            "Batch processing time: 270.94 seconds\n",
            "Batch processing time: 284.35 seconds\n",
            "Batch processing time: 297.84 seconds\n",
            "Batch processing time: 311.50 seconds\n",
            "Batch processing time: 325.50 seconds\n",
            "Batch processing time: 339.60 seconds\n",
            "Batch processing time: 353.71 seconds\n",
            "Batch processing time: 366.89 seconds\n",
            "Batch processing time: 380.33 seconds\n",
            "Batch processing time: 393.59 seconds\n",
            "Batch processing time: 407.27 seconds\n",
            "Batch processing time: 420.96 seconds\n",
            "Batch processing time: 434.60 seconds\n",
            "Batch processing time: 447.95 seconds\n",
            "Batch processing time: 461.67 seconds\n",
            "Batch processing time: 475.60 seconds\n",
            "Batch processing time: 489.32 seconds\n",
            "Batch processing time: 503.00 seconds\n",
            "Batch processing time: 515.71 seconds\n",
            "Batch processing time: 528.89 seconds\n",
            "Batch processing time: 542.77 seconds\n",
            "Batch processing time: 556.38 seconds\n",
            "Batch processing time: 569.84 seconds\n",
            "Batch processing time: 583.12 seconds\n",
            "Batch processing time: 596.50 seconds\n",
            "Batch processing time: 610.16 seconds\n",
            "Batch processing time: 624.03 seconds\n",
            "Batch processing time: 638.09 seconds\n",
            "Batch processing time: 651.01 seconds\n",
            "Batch processing time: 664.39 seconds\n",
            "Batch processing time: 677.66 seconds\n",
            "Batch processing time: 690.93 seconds\n",
            "Batch processing time: 704.29 seconds\n",
            "Batch processing time: 717.52 seconds\n",
            "Batch processing time: 730.69 seconds\n",
            "Batch processing time: 744.30 seconds\n",
            "Batch processing time: 757.93 seconds\n",
            "Batch processing time: 770.80 seconds\n",
            "Batch processing time: 783.65 seconds\n",
            "Batch processing time: 796.26 seconds\n",
            "Batch processing time: 809.10 seconds\n",
            "Batch processing time: 822.06 seconds\n",
            "Batch processing time: 835.40 seconds\n",
            "Batch processing time: 849.19 seconds\n",
            "Batch processing time: 862.54 seconds\n",
            "Batch processing time: 875.59 seconds\n",
            "Batch processing time: 888.57 seconds\n",
            "Batch processing time: 901.15 seconds\n",
            "Batch processing time: 914.24 seconds\n",
            "Batch processing time: 927.51 seconds\n",
            "Batch processing time: 940.78 seconds\n",
            "Batch processing time: 954.02 seconds\n",
            "Batch processing time: 967.51 seconds\n",
            "Batch processing time: 981.17 seconds\n",
            "Batch processing time: 994.51 seconds\n",
            "Batch processing time: 1007.98 seconds\n",
            "Batch processing time: 1020.73 seconds\n",
            "Batch processing time: 1033.19 seconds\n",
            "Batch processing time: 1046.96 seconds\n",
            "Batch processing time: 1060.37 seconds\n",
            "Batch processing time: 1073.44 seconds\n",
            "Batch processing time: 1086.72 seconds\n",
            "Batch processing time: 1100.55 seconds\n",
            "Batch processing time: 1114.14 seconds\n",
            "Batch processing time: 1128.09 seconds\n",
            "Batch processing time: 1141.18 seconds\n",
            "Batch processing time: 1153.80 seconds\n",
            "Batch processing time: 1166.84 seconds\n",
            "Batch processing time: 1180.38 seconds\n",
            "Batch processing time: 1193.82 seconds\n",
            "Batch processing time: 1207.42 seconds\n",
            "Batch processing time: 1220.49 seconds\n",
            "Batch processing time: 1233.82 seconds\n",
            "Batch processing time: 1247.66 seconds\n",
            "Batch processing time: 1261.39 seconds\n",
            "Batch processing time: 1274.99 seconds\n",
            "Batch processing time: 1288.16 seconds\n",
            "Batch processing time: 1300.92 seconds\n",
            "Batch processing time: 1314.44 seconds\n",
            "Batch processing time: 1328.14 seconds\n",
            "Batch processing time: 1341.78 seconds\n",
            "Batch processing time: 1355.05 seconds\n",
            "Batch processing time: 1368.78 seconds\n",
            "Batch processing time: 1382.19 seconds\n",
            "Batch processing time: 1395.47 seconds\n",
            "Batch processing time: 1408.70 seconds\n",
            "Batch processing time: 1421.60 seconds\n",
            "Batch processing time: 1434.82 seconds\n",
            "Batch processing time: 1448.49 seconds\n",
            "Batch processing time: 1461.65 seconds\n",
            "Batch processing time: 1474.73 seconds\n",
            "Batch processing time: 1488.37 seconds\n",
            "Batch processing time: 1501.64 seconds\n",
            "Batch processing time: 1515.54 seconds\n",
            "Batch processing time: 1528.69 seconds\n",
            "Batch processing time: 1541.05 seconds\n",
            "Batch processing time: 1554.26 seconds\n",
            "Batch processing time: 1567.90 seconds\n",
            "Batch processing time: 1580.93 seconds\n",
            "Batch processing time: 1593.90 seconds\n",
            "Batch processing time: 1606.88 seconds\n",
            "Batch processing time: 1620.36 seconds\n",
            "Batch processing time: 1634.19 seconds\n",
            "Batch processing time: 1647.13 seconds\n",
            "Batch processing time: 1660.04 seconds\n",
            "Batch processing time: 1673.12 seconds\n",
            "Batch processing time: 1686.09 seconds\n",
            "Batch processing time: 1699.41 seconds\n",
            "Batch processing time: 1712.58 seconds\n",
            "Batch processing time: 1725.94 seconds\n",
            "Batch processing time: 1739.23 seconds\n",
            "Batch processing time: 1752.70 seconds\n",
            "Batch processing time: 1766.01 seconds\n",
            "Batch processing time: 1779.05 seconds\n",
            "Batch processing time: 1792.07 seconds\n",
            "Batch processing time: 1805.15 seconds\n",
            "Batch processing time: 1818.59 seconds\n",
            "Batch processing time: 1831.91 seconds\n",
            "Batch processing time: 1844.91 seconds\n",
            "Batch processing time: 1858.65 seconds\n",
            "Batch processing time: 1872.46 seconds\n",
            "Batch processing time: 1886.29 seconds\n",
            "Batch processing time: 1900.15 seconds\n",
            "Batch processing time: 1913.45 seconds\n",
            "Batch processing time: 1926.55 seconds\n",
            "Batch processing time: 1939.87 seconds\n",
            "Batch processing time: 1952.99 seconds\n",
            "Batch processing time: 1967.12 seconds\n",
            "Batch processing time: 1980.47 seconds\n",
            "Batch processing time: 1993.72 seconds\n",
            "Batch processing time: 2007.32 seconds\n",
            "Batch processing time: 2020.96 seconds\n",
            "Batch processing time: 2035.11 seconds\n",
            "Batch processing time: 2049.16 seconds\n",
            "Batch processing time: 2062.16 seconds\n",
            "Batch processing time: 2075.21 seconds\n",
            "Batch processing time: 2088.25 seconds\n",
            "Batch processing time: 2101.90 seconds\n",
            "Batch processing time: 2115.95 seconds\n",
            "Batch processing time: 2129.30 seconds\n",
            "Batch processing time: 2142.18 seconds\n",
            "Batch processing time: 2155.38 seconds\n",
            "Batch processing time: 2169.16 seconds\n",
            "Batch processing time: 2182.49 seconds\n",
            "Batch processing time: 2196.35 seconds\n",
            "Batch processing time: 2208.86 seconds\n",
            "Batch processing time: 2221.57 seconds\n",
            "Batch processing time: 2235.44 seconds\n",
            "Batch processing time: 2248.74 seconds\n",
            "Batch processing time: 2261.92 seconds\n",
            "Batch processing time: 2275.44 seconds\n",
            "Batch processing time: 2288.99 seconds\n",
            "Batch processing time: 2302.90 seconds\n",
            "Batch processing time: 2316.41 seconds\n",
            "Batch processing time: 2329.63 seconds\n",
            "Batch processing time: 2343.04 seconds\n",
            "Batch processing time: 2356.59 seconds\n",
            "Batch processing time: 2370.38 seconds\n",
            "Batch processing time: 2384.22 seconds\n",
            "Batch processing time: 2398.06 seconds\n",
            "Batch processing time: 2411.20 seconds\n",
            "Batch processing time: 2424.52 seconds\n",
            "Batch processing time: 2437.95 seconds\n",
            "Batch processing time: 2451.65 seconds\n",
            "Batch processing time: 2465.83 seconds\n",
            "Batch processing time: 2478.98 seconds\n",
            "Batch processing time: 2491.75 seconds\n",
            "Batch processing time: 2505.08 seconds\n",
            "Batch processing time: 2518.28 seconds\n",
            "Batch processing time: 2531.67 seconds\n",
            "Batch processing time: 2544.96 seconds\n",
            "Batch processing time: 2558.05 seconds\n",
            "Batch processing time: 2571.72 seconds\n",
            "Batch processing time: 2585.59 seconds\n",
            "Batch processing time: 2598.72 seconds\n",
            "Batch processing time: 2611.16 seconds\n",
            "Batch processing time: 2624.52 seconds\n",
            "Batch processing time: 2637.98 seconds\n",
            "Batch processing time: 2651.22 seconds\n",
            "Batch processing time: 2664.71 seconds\n",
            "Batch processing time: 2677.87 seconds\n",
            "Batch processing time: 2691.65 seconds\n",
            "Batch processing time: 2705.56 seconds\n",
            "Batch processing time: 2718.74 seconds\n",
            "Batch processing time: 2731.24 seconds\n",
            "Batch processing time: 2744.53 seconds\n",
            "Batch processing time: 2758.62 seconds\n",
            "Batch processing time: 2772.11 seconds\n",
            "Batch processing time: 2785.60 seconds\n",
            "Batch processing time: 2799.36 seconds\n",
            "Batch processing time: 2813.02 seconds\n",
            "Batch processing time: 2826.96 seconds\n",
            "Batch processing time: 2840.71 seconds\n",
            "Batch processing time: 2854.23 seconds\n",
            "Batch processing time: 2866.87 seconds\n",
            "Batch processing time: 2879.69 seconds\n",
            "Batch processing time: 2893.25 seconds\n",
            "Batch processing time: 2906.90 seconds\n",
            "Batch processing time: 2920.06 seconds\n",
            "Batch processing time: 2933.43 seconds\n",
            "Batch processing time: 2947.15 seconds\n",
            "Batch processing time: 2960.79 seconds\n",
            "Batch processing time: 2974.16 seconds\n",
            "Batch processing time: 2987.61 seconds\n",
            "Batch processing time: 3000.32 seconds\n",
            "Batch processing time: 3012.89 seconds\n",
            "Batch processing time: 3026.61 seconds\n",
            "Batch processing time: 3039.83 seconds\n",
            "Batch processing time: 3052.69 seconds\n",
            "Batch processing time: 3066.13 seconds\n",
            "Batch processing time: 3079.39 seconds\n",
            "Batch processing time: 3092.84 seconds\n",
            "Batch processing time: 3105.73 seconds\n",
            "Batch processing time: 3118.52 seconds\n",
            "Batch processing time: 3131.52 seconds\n",
            "Batch processing time: 3145.21 seconds\n",
            "Batch processing time: 3158.64 seconds\n",
            "Batch processing time: 3171.73 seconds\n",
            "Batch processing time: 3185.34 seconds\n",
            "Batch processing time: 3199.15 seconds\n",
            "Batch processing time: 3212.51 seconds\n",
            "Batch processing time: 3226.23 seconds\n",
            "Batch processing time: 3239.54 seconds\n",
            "Batch processing time: 3251.97 seconds\n",
            "Batch processing time: 3265.10 seconds\n",
            "Batch processing time: 3278.22 seconds\n",
            "Batch processing time: 3291.22 seconds\n",
            "Batch processing time: 3304.32 seconds\n",
            "Batch processing time: 3317.89 seconds\n",
            "Batch processing time: 3331.24 seconds\n",
            "Batch processing time: 3344.82 seconds\n",
            "Batch processing time: 3358.32 seconds\n",
            "Batch processing time: 3371.15 seconds\n",
            "Batch processing time: 3384.47 seconds\n",
            "Batch processing time: 3397.97 seconds\n",
            "Batch processing time: 3411.27 seconds\n",
            "Batch processing time: 3424.58 seconds\n",
            "Batch processing time: 3437.76 seconds\n",
            "Batch processing time: 3451.32 seconds\n",
            "Batch processing time: 3465.31 seconds\n",
            "Batch processing time: 3479.13 seconds\n",
            "Batch processing time: 3492.46 seconds\n",
            "Batch processing time: 3505.23 seconds\n",
            "Batch processing time: 3518.01 seconds\n",
            "Batch processing time: 3531.38 seconds\n",
            "Batch processing time: 3544.85 seconds\n",
            "Batch processing time: 3557.97 seconds\n",
            "Batch processing time: 3571.55 seconds\n",
            "Batch processing time: 3585.34 seconds\n",
            "Batch processing time: 3598.56 seconds\n",
            "Batch processing time: 3611.59 seconds\n",
            "Batch processing time: 3624.53 seconds\n",
            "Batch processing time: 3637.10 seconds\n",
            "Batch processing time: 3650.67 seconds\n",
            "Batch processing time: 3663.89 seconds\n",
            "Batch processing time: 3677.32 seconds\n",
            "Batch processing time: 3690.57 seconds\n",
            "Batch processing time: 3703.78 seconds\n",
            "Batch processing time: 3717.45 seconds\n",
            "Batch processing time: 3730.85 seconds\n",
            "Batch processing time: 3743.72 seconds\n",
            "Batch processing time: 3756.99 seconds\n",
            "Batch processing time: 3770.53 seconds\n",
            "Batch processing time: 3783.97 seconds\n",
            "Batch processing time: 3797.33 seconds\n",
            "Batch processing time: 3810.76 seconds\n",
            "Batch processing time: 3824.01 seconds\n",
            "Batch processing time: 3837.39 seconds\n",
            "Batch processing time: 3850.71 seconds\n",
            "Batch processing time: 3863.89 seconds\n",
            "Batch processing time: 3876.93 seconds\n",
            "Batch processing time: 3889.73 seconds\n",
            "Batch processing time: 3902.67 seconds\n",
            "Batch processing time: 3915.93 seconds\n",
            "Batch processing time: 3928.98 seconds\n",
            "Batch processing time: 3942.40 seconds\n",
            "Batch processing time: 3955.76 seconds\n",
            "Batch processing time: 3968.85 seconds\n",
            "Batch processing time: 3982.68 seconds\n",
            "Batch processing time: 3995.18 seconds\n",
            "Batch processing time: 4008.24 seconds\n",
            "Batch processing time: 4021.86 seconds\n",
            "Batch processing time: 4035.44 seconds\n",
            "Batch processing time: 4048.72 seconds\n",
            "Batch processing time: 4061.61 seconds\n",
            "Batch processing time: 4075.02 seconds\n",
            "Batch processing time: 4088.46 seconds\n",
            "Batch processing time: 4101.97 seconds\n",
            "Batch processing time: 4114.70 seconds\n",
            "Batch processing time: 4127.70 seconds\n",
            "Batch processing time: 4140.66 seconds\n",
            "Batch processing time: 4154.05 seconds\n",
            "Batch processing time: 4167.34 seconds\n",
            "Batch processing time: 4180.37 seconds\n",
            "Batch processing time: 4193.67 seconds\n",
            "Batch processing time: 4207.31 seconds\n",
            "Batch processing time: 4220.88 seconds\n",
            "Batch processing time: 4234.25 seconds\n",
            "Batch processing time: 4247.47 seconds\n",
            "Batch processing time: 4259.66 seconds\n",
            "Batch processing time: 4273.22 seconds\n",
            "Batch processing time: 4286.31 seconds\n",
            "Batch processing time: 4298.94 seconds\n",
            "Batch processing time: 4312.37 seconds\n",
            "Batch processing time: 4325.96 seconds\n",
            "Batch processing time: 4339.41 seconds\n",
            "Batch processing time: 4352.19 seconds\n",
            "Batch processing time: 4365.07 seconds\n",
            "Batch processing time: 4377.57 seconds\n",
            "Batch processing time: 4390.62 seconds\n",
            "Batch processing time: 4403.77 seconds\n",
            "Batch processing time: 4416.48 seconds\n",
            "Batch processing time: 4429.88 seconds\n",
            "Batch processing time: 4443.02 seconds\n",
            "Batch processing time: 4455.38 seconds\n",
            "Batch processing time: 4467.47 seconds\n",
            "Batch processing time: 4480.35 seconds\n",
            "Batch processing time: 4493.63 seconds\n",
            "Batch processing time: 4506.36 seconds\n",
            "Batch processing time: 4519.61 seconds\n",
            "Batch processing time: 4532.64 seconds\n",
            "Batch processing time: 4545.74 seconds\n",
            "Batch processing time: 4557.90 seconds\n",
            "Batch processing time: 4570.60 seconds\n",
            "Batch processing time: 4583.20 seconds\n",
            "Batch processing time: 4595.74 seconds\n",
            "Batch processing time: 4608.88 seconds\n",
            "Batch processing time: 4622.02 seconds\n",
            "Batch processing time: 4634.86 seconds\n",
            "Batch processing time: 4647.48 seconds\n",
            "Batch processing time: 4660.44 seconds\n",
            "Batch processing time: 4673.90 seconds\n",
            "Batch processing time: 4687.03 seconds\n",
            "Batch processing time: 4700.25 seconds\n",
            "Batch processing time: 4713.38 seconds\n",
            "Batch processing time: 4726.96 seconds\n",
            "Batch processing time: 4740.32 seconds\n",
            "Batch processing time: 4753.08 seconds\n",
            "Batch processing time: 4765.35 seconds\n",
            "Batch processing time: 4778.23 seconds\n",
            "Batch processing time: 4791.13 seconds\n",
            "Batch processing time: 4803.82 seconds\n",
            "Batch processing time: 4816.72 seconds\n",
            "Batch processing time: 4829.83 seconds\n",
            "Batch processing time: 4842.94 seconds\n",
            "Batch processing time: 4855.97 seconds\n",
            "Batch processing time: 4868.33 seconds\n",
            "Batch processing time: 4881.62 seconds\n",
            "Batch processing time: 4894.88 seconds\n",
            "Batch processing time: 4907.97 seconds\n",
            "Batch processing time: 4921.13 seconds\n",
            "Batch processing time: 4934.65 seconds\n",
            "Batch processing time: 4948.07 seconds\n",
            "Batch processing time: 4961.66 seconds\n",
            "Batch processing time: 4974.91 seconds\n",
            "Batch processing time: 4987.61 seconds\n",
            "Batch processing time: 5000.76 seconds\n",
            "Batch processing time: 5013.37 seconds\n",
            "Batch processing time: 5026.32 seconds\n",
            "Batch processing time: 5039.38 seconds\n",
            "Batch processing time: 5052.10 seconds\n",
            "Batch processing time: 5065.34 seconds\n",
            "Batch processing time: 5078.57 seconds\n",
            "Batch processing time: 5090.88 seconds\n",
            "Batch processing time: 5103.05 seconds\n",
            "Batch processing time: 5116.24 seconds\n",
            "Batch processing time: 5129.57 seconds\n",
            "Batch processing time: 5142.33 seconds\n",
            "Batch processing time: 5155.70 seconds\n",
            "Batch processing time: 5168.73 seconds\n",
            "Batch processing time: 5181.73 seconds\n",
            "Batch processing time: 5194.19 seconds\n",
            "Batch processing time: 5206.42 seconds\n",
            "Batch processing time: 5219.74 seconds\n",
            "Batch processing time: 5233.17 seconds\n",
            "Batch processing time: 5246.25 seconds\n",
            "Batch processing time: 5259.72 seconds\n",
            "Batch processing time: 5273.30 seconds\n",
            "Batch processing time: 5286.49 seconds\n",
            "Batch processing time: 5299.62 seconds\n",
            "Batch processing time: 5311.71 seconds\n",
            "Batch processing time: 5324.58 seconds\n",
            "Batch processing time: 5337.70 seconds\n",
            "Batch processing time: 5350.65 seconds\n",
            "Batch processing time: 5363.79 seconds\n",
            "Batch processing time: 5376.70 seconds\n",
            "Batch processing time: 5389.98 seconds\n",
            "Batch processing time: 5402.37 seconds\n",
            "Batch processing time: 5414.78 seconds\n",
            "Batch processing time: 5427.94 seconds\n",
            "Batch processing time: 5440.57 seconds\n",
            "Batch processing time: 5453.52 seconds\n",
            "Batch processing time: 5466.96 seconds\n",
            "Batch processing time: 5480.01 seconds\n",
            "Batch processing time: 5492.94 seconds\n",
            "Batch processing time: 5505.75 seconds\n",
            "Batch processing time: 5518.65 seconds\n",
            "Batch processing time: 5531.92 seconds\n",
            "Batch processing time: 5544.96 seconds\n",
            "Batch processing time: 5557.86 seconds\n",
            "Batch processing time: 5571.07 seconds\n",
            "Batch processing time: 5584.62 seconds\n",
            "Batch processing time: 5597.77 seconds\n",
            "Batch processing time: 5610.62 seconds\n",
            "Batch processing time: 5623.06 seconds\n",
            "Batch processing time: 5635.84 seconds\n",
            "Batch processing time: 5648.70 seconds\n",
            "Batch processing time: 5661.75 seconds\n",
            "Batch processing time: 5674.99 seconds\n",
            "Batch processing time: 5688.61 seconds\n",
            "Batch processing time: 5701.87 seconds\n",
            "Batch processing time: 5714.73 seconds\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "# Define the batch size\n",
        "batch_size = 4\n",
        "\n",
        "# Define the custom dataset class\n",
        "class CaptionDataset(Dataset):\n",
        "    def __init__(self, train, mapping, features, tokenizer, max_length):\n",
        "        self.train = train\n",
        "        self.mapping = mapping\n",
        "        self.features = features\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        id = self.train[idx]\n",
        "        img = self.features[id]\n",
        "        caps = self.mapping[id]\n",
        "        seq = self.tokenizer.batch_encode_plus(caps, max_length=self.max_length, padding='max_length', truncation=True)\n",
        "        seq = torch.tensor(seq['input_ids'])\n",
        "        return seq  # Removed img from return statement\n",
        "\n",
        "# Create the dataset and data loader\n",
        "dataset = CaptionDataset(train, mapping, features, tokenizer, max_length)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "# Define the model\n",
        "# Define the model\n",
        "class CaptionModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CaptionModel, self).__init__()\n",
        "        self.decoder = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "    def forward(self, cap):\n",
        "      if self.training:\n",
        "          outputs = self.decoder(input_ids=cap.view(-1, cap.size(-1)), labels=cap.view(-1, cap.size(-1)))\n",
        "          return outputs.logits[:, :-1, :].contiguous(), cap.view(-1, cap.size(-1))[:, 1:].contiguous()\n",
        "      else:\n",
        "          batch_size, seq_length = cap.shape\n",
        "          outputs = self.decoder.generate(input_ids=cap.unsqueeze(1), max_length=seq_length)\n",
        "          return outputs\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "model = CaptionModel()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(3):\n",
        "    print(f\"Epoch {epoch+1}/3\")\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        batch = batch.squeeze()\n",
        "        outputs, targets = model(batch)\n",
        "        targets = torch.nn.functional.one_hot(targets, num_classes=32128).float()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Batch processing time: {time.time() - start_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgG62fqNCOPI"
      },
      "outputs": [],
      "source": [
        "# save the model\n",
        "model.save(WORKING_DIR+'/RSICD__RESNET_best_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssxDoz2VSCuh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}